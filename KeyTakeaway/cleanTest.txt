Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph University of Science and Technology University of Science and Technology University of Science and Technology University of Science and Technology University of Science and Technology University of Science and Technology Temporal Knowledge Graph (TKG) reasoning often involves com- pleting missing factual elements along the timeline. Although exist- ing methods can learn good embeddings for each factual element in quadruples by integrating temporal information, they often fail to infer the evolution of temporal facts. This is mainly because of (1) insufficiently exploring the internal structure and semantic relationships within individual quadruples and (2) inadequately learning a unified representation of the contextual and temporal correlations among different quadruples. To overcome these limi- tations, we propose a novel Transformer-based reasoning model (dubbed ECEformer) for TKG to learn the Evolutionary Chain of Events (ECE). Specifically, we unfold the neighborhood subgraph of an entity node in chronological order, forming an evolutionary chain of events as the input for our model. Subsequently, we utilize a Transformer encoder to learn the embeddings of intra-quadruples for ECE. We then craft a mixed-context reasoning module based on the multi-layer perceptron (MLP) to learn the unified represen- tations of inter-quadruples for ECE while accomplishing temporal knowledge reasoning. In addition, to enhance the timeliness of the events, we devise an additional time prediction task to complete effective temporal information within the learned unified represen- tation. Extensive experiments on six benchmark datasets verify the state-of-the-art performance and the effectiveness of our method. âˆ—Both authors contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. â€¢ Computing methodologies â†’Temporal reasoning. Temporal Knowledge Graph Completion, Context Information Min- ing, Link Prediction, Evolutionary Chain of Event Zhiyu Fang, Shuai-Long Lei, Xiaobin Zhu, Chun Yang, Shi-Xue Zhang, Xu-Cheng Yin, and Jingyan Qin. 2024. Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR â€™24), July 14â€“18, 2024, Washing- ton, DC, USA. ACM, New York, NY, USA, 10 pages. / Knowledge graphs (KGs) store human-summarized prior knowl- edge in structured data formats and are widely applied in many downstream tasks, such as information retrieval [2], intelligent question answering [3], and recommendation systems [44]. Tra- ditional knowledge graphs employ triples (ğ‘ , ğ‘,ğ‘œ) to record each fact or event in the knowledge base, where ğ‘ and ğ‘œrespectively represent the subject and object entities, and ğ‘denotes the logical predicate (or relation) connecting these two entities. e.g., (Barack Obama, presidentOf, USA). However, events in the real world often dynamically evolve over time. To extend the temporal nature of events, Temporal Knowledge Graphs (TKGs) incorporate tempo- ral information into the traditional triples, resulting in quadruples (ğ‘ , ğ‘,ğ‘œ,ğœ). e.g., (Barack Obama, presidentOf, USA, 2009â€“2017) and (Donald Trump, presidentOf, USA, 2017â€“2021). Completing missing events for specific timestamps in TKGs via Temporal Knowledge Graph Reasoning (TKGR) has recently at- tracted growing attention from both academic and industrial com- munities. For example, the missing element Barack Obama in the quadruple query (?, president of, USA, 2009â€“2017) can be predicted via TKGR. Existing TKGR methods can be broadly categorized into two types according to the temporal scope of link prediction, arXiv:2405.00352v1 [cs.AI] 1 May 2024 SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Figure 1: Illustration of the overall concept of our method. Part (a) displays a subgraph of TKG, which includes facts related to three U.S. Presidents. Part (b) illustrates our moti- vation for the ECEformer to learn a unified representation from an evolutionary chain of events, thereby simultane- ously addressing link and time prediction tasks. i.e., interpolation and extrapolation [16]. The former focuses on completing unknown knowledge in the past, whereas the latter concentrates on estimating unknown knowledge in the future. Con- cretely, interpolation reasoning methods aim to effectively integrate temporal information into the evolutionary trajectory of events. They typically utilize geographic and geometric information to model the relationships among elements within individual quadru- ples. Accordingly, HyTE [8] employs a translation-based strategy [4] to measure the distance between entities and relations in the customized hyperplane, where each timestamp is associated with a corresponding hyperplane. Inspised by BoxE [1], BoxTE [27] ad- ditionally incorporates temporal information embedded through the relation-specific transition matrix within the foundational box- type relation modeling. TGeomE [41], a geometric algebra-based embedding approach, performs 4th-order tensor factorization and applies linear temporal regularization for time representation learn- ing. These methods impose a rigid prior on KGs, relying solely on geometric modeling and geographic measurements. However, they ignore the implicit semantic information, failing to fully capture the complex graph structure and relational context. Therefore, it is necessary to sufficiently explore the internal structure and semantic relationships within individual quadruples (Limi- On the other hand, extrapolation reasoning methods aim to effectively leverage the query-associated historical information [36]. They generally utilize Graph Neural Network (GNN) mod- els to capture structural characteristics within each snapshot and leverage Recurrent Neural Networks (RNNs) to explore the evo- lutionary characteristics across timestamps. Accordingly, RE-Net [16] employs a multi-relational graph aggregator to capture the graph structure and an RNN-based encoder to capture temporal dependencies. RE-GCN [24] leverages a relation-aware Graph Con- volution Network (GCN) to capture the structural dependencies for the evolution unit and simultaneously captures the sequential patterns of all facts in parallel using gate recurrent components. RPC [25] mines the information underlying the relational corre- lations and periodic patterns via two correspondence units: one is based on a relational GCN, and the other is based on Gated Re- current Units (GRUs). However, this phased processing strategy can easily degrade the accuracy of predicting future events due to incorrect historical information. Therefore, it is necessary to adap- tively correct the interference of incorrect historical information on prediction accuracy by learning a unified representation of the contextual and temporal correlations among different Given a subgraph of TKG (as shown in Figure 1a), interpolation methods utilize geographical information to maximize structural differences between different quadruples. However, this type of approaches cannot effectively utilize semantic information in rea- soning. Moreover, learning discriminative and powerful representa- tions for each item within a quadruple is challenging, especially in modeling timestamps. For instance, differentiating between (Barack Obama, visitTo, France, 2009) and (Donald Trump, visitTo, France, 2017) necessitates precise characterizations of different names and timestamps. Although extrapolation methods consider neighbor- hood information, independently processing structural and tempo- ral characteristics can lead to confusion in reasoning. For instance, under the timestamps of 2009 and 2017, the structure of Barack Obama and Donald Trump with USA and France is consistent. Hence, the model can naturally infer the structure of George Bush with USA and France in 2001 should be consistent with the structure of Barack Obama and Donald Trump, which contradicts the facts. From this key observation, we believe that designing an end-to-end network for learning a unified representation rich in information from temporal subgraph inputs will enhance the performance of TKGR. This network adaptively extracts both intra-quadruple and inter-quadruple, together with temporal information. According to the aforementioned analysis, we propose an inno- vative end-to-end Transformer-based network to learn the evolu- tionary chain of events (dubbed ECEformer) for TKGR. Figure 1b illustrates the motivation of our method. To easily illustrate the structural and contextual relationship between the query node and the adjacent nodes, we form an evolutionary chain of events (ECE) by extracting query-specific neighbors from the subgraph. Subse- quently, our proposed Transformer-based model learns the unified representation for the ECE via two novel modules, namely, ECE Representation Learning (ECER) and Mixed-Context Knowledge Reasoning (MCKR). Specifically, to overcome Limitation I, ECER employs a Transformer-based encoder to embed each event in the ECE, such as (USA, president of, 2001) corresponding to a specific query George Bush. To overcome Limitation II, MCKR induces the embeddings of each event and enhances interaction within and between quadruples via crafting an MLP-based information mixing layer. In addition, to enhance the timeliness of the events, we de- vise an additional time prediction task to imbue effective temporal information within the learned unified representation. Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA In summary, our main contributions are three-fold: â€¢ We propose an innovative temporal knowledge graph rea- soning method, namely ECEformer. Experimental results verify the state-of-the-art performance of our method on six â€¢ Proposing a Transformer-based encoder for ECE representa- tion learning (ECER) aims to explore the internal structure and semantic relationships within individual quadruples. â€¢ Proposing an MLP-based layer for Mixed-Context Knowl- edge Reasoning (MCKR) aims to learn a unified represen- tation of the contextual and temporal correlations among Interpolation-based TKGR methods estimate missing facts or events by identifying consistent trends within a TKG. Based on the process strategy for temporal information, we categorize them into two types: timestamp-independent and timestamp-specific methods. Timestamp-independent methods treat timestamps as indepen- dent units, equivalent to entities or relations, and do not apply additional operations on the timestamps. Generally, these methods [19, 20, 27, 29] directly associate timestamps to the corresponding entities and relations based on the foundation of static KG models. For example, Leblay et al. [20] extended the classic TransE [4] model to TKGR by concatenating timestamp embeddings with relation embeddings. Inspired by BoxE [1], Messner et al. [27] directly in- troduced timestamp embeddings on its basis. However, timestamp- independent methods are usually limited in capturing the temporal information of evolutionary events. Timestamp-specific methods embed the temporal information and learn the evolution of enti- ties and relations via timestamp-specific functions. To effectively utilize the structural and semantic information of the time, they ingeniously designed various time-specific functions, such as di- achronic embedding functions [11], time-rotating functions [5, 40], time-hyperplane functions [8, 38, 41, 47], and non-linear embedding functions [12, 17, 22, 35, 39]. Specifically, Goel et al. [11] proposed a diachronic entity embedding function to characterize entities at any timestamp. Chen et al. [5] defined the spatial rotation oper- ation of entities around the time axis. Zhang et al. [47] learned spatial structures interactively between the Euclidean, hyperbolic and hyper-spherical spaces. Han et al. [12] explored evolving entity representations on a mixed curvature manifold using a velocity vector defined in the tangent space at each timestamp. Despite the great efforts made by these studies, they are limited in predicting Extrapolation-based TKGR methods predict future facts or events by learning effective embeddings from historical snapshots. As knowledge subgraphs at specific timestamps, historical snapshots inherently contain rich structural and semantic information. Ex- trapolation methods typically employ deep learning techniques such as Graph Neural Networks (GNNs) [24, 45, 46], Recurrent Neural Networks (RNNs) [16, 21], and Reinforcement Learning (RL) [33, 49] to extract features from historical snapshots. To elaborate further, GNN-based methods are naturally applied to TKGR due to the inherent topological structure of historical snapshots. Schlichtkrull et al. [30] early utilized Graph Convolu- tional Networks (GCN) to model the multi-relational features of traditional KG. To incorporate the temporal characteristics in TKGs, Jin et al. [16] additionally introduced an RNN-based sub-network specifically for recursively encoding past facts. Li et al. [24] pre- sented the recurrent evolution network based on GCN, which learns the evolutionary representations of entities and relations at each timestamp by modeling the KG sequence recurrently. To handle the large-scale inputs and non-independent and identically distributed data, Deng et al. [9] combined GCN and GRU for simultaneously predicting concurrent events of multiple types and inferring multi- ple candidate participants. RL-based methods are becoming popular to improve the interpretability of reasoning methods, especially in question-answering tasks. By mimicking the human mechanism of searching for historical information and meticulous reasoning, CluSTeR [23] implements an interpretable prediction for TKGR using a two-stage approach of clue searching and temporal reason- ing. DREAM [49] introduces an adaptive reinforcement learning model based on attention mechanism, addressing the challenges in traditional RL-based TKG reasoning, specifically the lack of captur- ing temporal evolution and semantic dependence jointly and the over-reliance on manually designed rewards. Furthermore, some other types of methods also achieve good performance in TKGR. For example, TLogic [26] introduces an explainable framework based on temporal logical rules, leveraging temporal random walks to provide explanations while maintaining time consistency. Faced with the challenge of predicting future facts for newly emerging entities based on extremely limited observational data, MetaTKGR [37] dynamically adjusts its strategies for sampling and aggregating neighbors from recent facts and introduces a temporal adaptation regularizer to stabilize the meta-temporal reasoning process over time. In summary, extrapolation methods generally process struc- tural and temporal characteristics independently. This approach ignores the intrinsic correlation between structural and temporal in- formation, limiting the capability to extract a unified representation With the Transformer achieving excellent performance in numer- ous tasks in natural language processing and computer vision, it has been introduced into knowledge graph tasks. Leveraging the powerful learning capability of Transformers for sequential data structures and contexts, numerous Transformer-based methods [6, 7, 14, 43] have emerged for traditional KGs. For example, Chen et al. [6] proposed two different Transformer blocks for hierarchi- cally learning representations of entities and relations. In addition, some methods also achieve remarkable results in downstream tasks of KGs. Specifically, Hu et al. [13] utilized a Transformer-based framework for encoding the content of neighbors of an entity for entity typing task. Meanwhile, Xu et al. [42] applied it for context- aware rule mining over KG. To serve diverse KG-related tasks, Zhang et al. [48] proposed a uniform knowledge representation and fusion module via structure pretraining and prompt tuning. SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Transformer enables the exploration of structural associations within each historical snapshot and captures the temporal rela- tionships among different historical snapshots to accomplish link prediction tasks [36]. GHT [32] involves two variants of Trans- former and a relational continuous-time encoding function, aiming to mitigate the interference of facts irrelevant to the query and enhancing the capability of the model to learn long-term evolu- tion. To effectively embed the rich information in the query-specific subgraph, SToKE [10] learns joint structural and temporal contextu- alized knowledge embeddings employing the pre-trained language model. Overview: While the Transformer framework has wide ap- plication in traditional KGs, its exploration in the field of TKGR Note that TKG G = (E, R, T, Q) denotes a directed multi-relation graph with timestamped edges between entities, where E âˆˆR| E|Ã—ğ‘‘, R âˆˆR|R|Ã—ğ‘‘, and T âˆˆR| T|Ã—ğ‘‘are the set of entities, relations and timestamps, respectively. Q = {(ğ‘ , ğ‘,ğ‘œ,ğœ) | ğ‘ , ğ‘œâˆˆE, ğ‘âˆˆR,ğœâˆˆT } is a set of quadruples in G, where ğ‘ , ğ‘œ, ğ‘and ğœare a subject entity, an object entity, the relation between them, and a timestamp, re- spectively. For the link prediction task, we infer the missing object entity by converting (ğ‘ , ğ‘,?,ğœ). Similarly, we infer the missing sub- ject entity by converting (ğ‘œ, ğ‘âˆ’1,?,ğœ), where we employ a reciprocal predicate ğ‘âˆ’1 to distinguish this case. Evolutionary Chain of Events. The evolutionary chain of events (ECE) transforms structural and temporal contexts related to the query-specific neighborhood subgraph into a structured knowledge sequence. Specifically, for the query (ğ‘ , ğ‘,?,ğœ), ECE is conceptualized as a singly linked list C = {ğ¶0,ğ¶1, Â· Â· Â·,ğ¶ğ‘˜}, compris- ing a chronologically arranged collection of events relevant to the subject ğ‘ of the query. Notably, each ğ¶represents an event related to ğ‘ , depicted by a triplet (ğ‘’, ğ‘,ğœ), where ğ¶0 signifies the query and ğ¶ğ‘˜denotes adjacent facts, ğ‘˜being the maximum adjacency count, ğ‘’can serve as either the subject entity or the object entity. For example, for the query (George Bush, visitTo,?, 2002) as shown in Figure 1a, the ECE can be formulated as: C ={(George Bush, visitTo, 2002), (USA, presidentOf, 2001), (Barack Obama, succeded, 2009)}. The formal expression is illustrated in Figure 1b. Hence, Our ECE can structurally represent both intra-quadruples and inter-quadruples. By encapsulating the dynamic evolution of entities and their inter- relations within the event chain, the ECE can capture the historical and current events related to the subject entity. As an innovative end-to-end Transformer-based network, ECE- former learns the evolutionary chain of events (ECE) for TKGR. This work defines the subgraph composed of adjacent nodes that evolve over time along with the query entity as the evolutionary chain of events (ECE). Different from existing Transformer-based models, ECEformer crafts specific modules for handling intra-quadruples and inter-quadruples, respectively. As for intra-quadruples, ECE representation learning (ECER) based on a Transformer encoder is designed to explore the internal structure and semantic relation- ship within individual quadruples from the input ECE. On the other hand, as for the inter-quadruples, mixed-context knowledge rea- soning (MCKR) involves a novel information mixing layer, and the iterative transposition mechanism is designed to explore the con- textual and temporal correlations among different quadruples. The architecture of ECEformer is shown in Figure 2, and more details will be illustrated in the following sections. The first key to effectively learning ECE lies in the comprehen- sive extraction and analysis of each triplet corresponding to the sub-nodes on the ECE. Every constituent node of the ECE stores a historical event associated with the central entity, necessitating an investigation into its inherent structural and temporal characteris- tics. Inspired by the effectiveness of the Transformer architecture in learning inter-data dependencies, we introduce an ECER mod- ule utilizing a Transformer encoder that distills the embedding of each triplet. The details are shown in Figure 2c. Specifically, ECER consists of ğ‘core units, each consisting of alternating lay- ers of multi-head attention (MHA), layer normalization (LN), and MLP-based feed-forward (FF). Among them, both MHA and FF are followed by a LN accompanied by the residual connection (RC). The FF contains two layers with the GELU non-linear activation function. This process can be formulated as: where â„,ğ‘™and ğ‘“denote MHA function with weightğ‘Šâ„, LN function with weight ğ‘Šğ‘™, and FF function with weight ğ‘Šğ‘“, respectively. ğ‘Ÿ denotes the RC function, which performs an operation that directly combines the input with the output of the intermediate layer. ğ‘¥ denotes the input sequence. ğ‘¦denotes the output embedding. â—¦ denotes the composite function operator. Note that the MHA relies on the scaled dot-product attention mechanism to learn semantic interactions between the current token and all tokens within the sequence, thereby thoroughly exploring the structural and seman- tic information within individual events. Additionally, by directly inputting timestamps as tokens into ECER, the model can semanti- cally extract temporal information within individual events. Given a query ğ‘(ğ‘ , ğ‘,?,ğœ) and its ECE Cğ‘, we treat each compo- nent within the branch of ECE as an individual token, sequentially unfolding the chain structure based on temporal order. This process yields a new input, as illustrated in Figure 2a. For each triplet on every branch of the ECE, we append a special token, i.e., [ğ¶ğ¿ğ‘†]. Subsequently, we employ the concatenation of semantic embed- dings ğ¸and positional embeddings ğ‘ƒin the embedding space to represent each input token, as illustrated in Figure 2b. This process ğ‘–ğ‘›ğ‘= ğ¸ğ‘–+ ğ‘ƒğ‘—, {ğ‘–âˆˆE âˆªR âˆªT, ğ‘—âˆˆ[0, 1, 2, 3]}, ğ‘–ğ‘›ğ‘denotes ğ‘–âˆ’ğ‘¡â„token in the input sequence, ğ¸ğ‘–denotes the feature embedding of ğ‘–âˆ’ğ‘¡â„token, and ğ‘ƒğ‘—denotes the position embedding. These embeddings are iteratively fed into the ECER module Fğ¸ğ¶ğ¸ğ‘…: R4Ã—ğ‘‘â†’Rğ‘‘. Eventually, the output consists of ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’,ğ‘–= 0, 1, Â· Â· Â·,ğ‘˜, which represent the structural and semantic information implied in the triples cor- Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA (a) Input triples of ECE (c) Core unit in ECER (d) Core unit in MCKR Figure 2: The architecture of the proposed ECEformer. Given an evolutionary chain of events, ECEformer obtains input sequences (detailed in (a)) from each branch of the chain and converts each token into a concatenation of semantic embeddings ğ¸and positional embeddings ğ‘ƒ(detailed in (b)). Subsequently, these branch embeddings ğ¸ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’are derived by the ECER, which is based on the Transformer encoder (detailed in (c)). The contextual information from different branch embeddings is then inductively processed by the MLP-based MCKR (detailed in (d)), culminating in a unified representation. After encoding through the ECER, the embedded representations have effectively abstracted the relevant information of individual events. We introduce the query-specific contextual information to further enhance the accuracy of reasoning. Inspired by [34], we introduce the MLP-based MCKR module to learn the multi- dimensional information interaction of the inputs by leveraging two mixing strategies. The details are illustrated in Figure 2d. Different from the single event input in ECER, the MCKR mod- ule processes the sequence of 1 + ğ‘˜embedding representations of ECE. This forms a two-dimensional real-valued input matrix M âˆˆR(1+ğ‘˜)Ã—ğ‘‘. The MCKR consists of ğ‘€core units, each consist- ing of two key mixing layers: the channel MLP and the patch MLP. Additionally, each mixing unit is preceded by a layer normaliza- tion and a transposition layer. The channel MLP operates on the columns of M, mapping each column from R1+ğ‘˜to R1+ğ‘˜. It aims to facilitate information interaction across different event embeddings within the same feature dimension. Conversely, the patch MLP operates on the rows of M, mapping each row from Rğ‘‘to Rğ‘‘. It aims to facilitate information interaction within the same event embedding across different feature dimensions. This process can Mâˆ—,ğ‘–= ğ‘€âˆ—,ğ‘–+ğ‘Š2ğœ(ğ‘Š1Norm(M)âˆ—,ğ‘–), for ğ‘–= 1, 2, Â· Â· Â·,ğ‘‘ M)ğ‘—,âˆ—), for ğ‘—= 1, 2, Â· Â· Â·,ğ‘˜+ 1 (4) where ğœis the non-linear activation function (GELU). Each MLP block in MCKR contains two fully-connected layers,ğ‘Š1 andğ‘Š2 are hidden weights in the channel MLP,ğ‘Š3 andğ‘Š4 are hidden weights Following the methodology outlined in [34], our MCKR employs the same parameter to translate every column (row) for channel MLP (patch MLP). This parameter sharing mechanism endows the network with positional invariance, enhancing its ability to proficiently process sequential data. Moreover, this approach also circumvents the substantial increase in model complexity that typ- ically accompanies dimensional augmentation. Subsequently, we leverage a global average pooling layer to distill the mixed-context ğ‘€, which can be formulated as: where Uğ¸ğ¶ğ¸denotes the unified representation of ECE. Given a quadruple with either the subject or the object missing, the task of link prediction can be accomplished by computing the similarity between Uğ¸ğ¶ğ¸and all candidate entities. Link prediction can be SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Table 1: Statistics of the Experimental Datasets. where ğ‘†ğ‘–ğ‘šdenotes the similarity function between Uğ¸ğ¶ğ¸and a candidate entity, ğ‘denotes the predicted result, obtained by select- ing the ğ‘¡ğ‘œğ‘âˆ’1 entity based on similarity scores. ğ‘(ğ‘’ğ‘”ğ‘¡|ğ‘) denotes the occurrence probability that ğ‘is the target entity ğ‘’ğ‘”ğ‘¡. Considering that events in TKGs often coincide with specific times- tamps, the unified representations learned from link prediction might be trivial solutions facilitated by contextual information. Such representations will introduce spurious correlations, because they only consider entities and relations while neglecting the tem- poral aspect of events. To address this issue, we propose a marked time prediction task to enhance the exploration of temporal infor- mation during the contextualization process. Specifically, we utilize a special token [ğ‘€ğ´ğ‘†ğ¾] to replace the timestamp ğœin the query- specific branch of ECE. In other words, we transform the ğ¶0(ğ‘ , ğ‘,ğœ) in ECE to ğ¶0(ğ‘ , ğ‘, [ğ‘€ğ´ğ‘†ğ¾]). This direct masking strategy intro- duces perturbations to the original timestamps, thereby prompting the model to learn contextual information. To promote the modelâ€™s capability of understanding temporal information, we train the model using masked inputs to recover the perturbed timestamp. This design can encourage the model to assimilate contextual infor- mation and deduce the occurrence time of the query-specific event. The methodology is analogous to link prediction, where we di- rectly utilize the unified representation Uğ¸ğ¶ğ¸to predict the correct timestamp. The prediction probability ğ‘(ğœğ‘”ğ‘¡|ğ‘) can be formulated where ğ‘(ğœğ‘”ğ‘¡|ğ‘) denotes the probability that predicted timestamp ğ‘ corresponds to the target timestamp ğœğ‘”ğ‘¡. For link prediction and time prediction tasks, we employ the cross- entropy function to calculate the loss during the training process. The loss function can be formulated as: where (ğ‘ , ğ‘,ğ‘œ,ğœ) âˆˆG represents the historical events in the training set, ğœ†weights the time prediction task. Additionally, for diversifying our training data and reducing memory cost, we implement an ECE sampling strategy similar to the edge dropout regularization [6]. This approach samples only a portion of the neighborhood information of the query subject and filters out the ground truth target entity from the sampled In this section, the experimental settings are first introduced from four aspects, including datasets, evaluation metrics, compared base- lines, and implementation details. Then, we comprehensively ana- lyze the proposed ECEformer also from three aspects, i.e., superi- Datasets and Evaluation Metrics. We evaluate the reasoning performance of our ECEformer on six TKG benchmark datasets, including GDELT, ICEWS05-15, ICEWS18, ICEWS14, YAGO11K, and Wikidata12K. According to [25], we also split the datasets of GDELT and ICEWS05-15/18/14 into train/valid/test by times- tamps. According to [8], we split the datasets of YAGO11K and Wikidata12K into train/valid/test by time intervals. The statistical information of datasets is shown in Table 1, in which Time rep- resents time granularity. Two widely used evaluation metrics are adopted to quantify the performance, namely ğ‘€ğ‘…ğ‘…and ğ»ğ‘–ğ‘¡ğ‘ @ğ‘˜. ğ‘€ğ‘…ğ‘…represents the mean reciprocal rank of the inferred true entity among the queried candidates, and ğ»ğ‘–ğ‘¡ğ‘ @ğ‘˜measures the propor- tion of instances where the true entity appears among the top ğ‘˜ Compared Baselines. Considering that different methods are validated on various datasets, we divided our experiments into two groups according to dataset size for a fair comparison. We compared with twenty state-of-the-art (SOTA) models. Specifically, the extrapolation baselines for GDELT, ICEWS05-15, and ICEWS18 consist of RL-based models (TITer [33], TLogic [26], and DREAM [49]), GNN-based models (RE-GCN [24], TiRGN [21], HGLS [46], ğ¿2TKG [45], and RPC [25]), and Transformer-based models (GHT [32] and SToKE [10]). The interpolation baselines chosen for smaller datasets (ICEWS14, YAGO11K, and Wikidata12K) include TimePlex [15], TeLM [38], TeRo [40], RotateQVS [5], SANe [22], NeuSTIP [31], QDN [35], LCGE [28], TGeomE [41], and HyIE [47]. Implementation Details. All experiments are conducted on a single NVIDIA RTX A6000. The implementation1 of our model is built upon the open-source framework LibKGE2. The dimension of the input embedding is set to 320. The hidden widths of the feed-forward layer inside ECER and the channel (patch) MLP inside MCKR are set to 1024. The maximum neighborhood of ECE is set to 50. We configure the core unit numbers for ECER and MCKR to be ğ‘= 3 and ğ‘€= 6, respectively. The activation function in our We train our model via the Adamax [18] with an initial learning rate of 0.01 and an L2 weight decay rate of 0.01. We employed the warm-up training strategy, wherein the learning rate commenced from zero and increased linearly during the initial 10% of the train- ing steps, followed by a linear decrease throughout the remaining training process. We train the model with a batch size of 512 for a maximum of 300 epochs, employing an early stopping mechanism based on MRR in the validation set. Detailed discussions on the effects of the weight coefficient ğœ†, the number of core units ğ‘and ğ‘€are in the subsection Sensitivity Analysis. Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Table 2: Performance of link prediction on GDELT, ICEWS05-15, and ICEWS18. The best results are marked in bold, the second results are marked by underlining. Table 3: Performance of link prediction task on ICEWS14, YAGO11K, and Wikidata12K. The best results are marked in bold, the second results are marked by underlining. We compare our model with 20 SOTA models ranging from 2020 to 2023 on six benchmark datasets. We report the result in Table 2 and Table 3, where the best performances are highlighted in bold, while the second-best are marked by underlining. A noteworthy observation is that our ECEformer significantly outperforms other baseline models across all datasets in terms of ğ‘€ğ‘…ğ‘…and ğ»ğ‘–ğ‘¡ğ‘ @ğ‘˜. Specifically, ECEformer achieves performance improvements over the second-best models by 14.09%, 6.09%, 5.70%, 5.00%, 0.40%, and 4.61% on ğ‘€ğ‘…ğ‘…, respectively. Additionally, it achieves gains of 9.72%, 12.61%, 11.18%, 8.51%, 1.03%, and 8.25% on ğ»ğ‘–ğ‘¡ğ‘ @1, respectively. This significant margin of improvement demonstrates the superior In particular, our model exhibits notable performance in four metrics on GDELT: ğ‘€ğ‘…ğ‘…, ğ»ğ‘–ğ‘¡ğ‘ @1, ğ»ğ‘–ğ‘¡ğ‘ @3, and ğ»ğ‘–ğ‘¡ğ‘ @10, achieving respective scores of 51.19%, 38.72%, 59.41%, and 71.10%. Compared with RL-based methods, our ECEformer outperforms DREAM by 23.09% in ğ‘€ğ‘…ğ‘…, by 9.42% in ğ»ğ‘–ğ‘¡ğ‘ @1, by 28.31% in ğ»ğ‘–ğ‘¡ğ‘ @3, and by 26.4% in ğ»ğ‘–ğ‘¡ğ‘ @10, respectively. Compared with GNN-based meth- ods, our ECEformer outperforms RPC by 28.78% in ğ‘€ğ‘…ğ‘…, by 24.30% in ğ»ğ‘–ğ‘¡ğ‘ @1, by 35.05% in ğ»ğ‘–ğ‘¡ğ‘ @3, and by 32.77% in ğ»ğ‘–ğ‘¡ğ‘ @10, respec- tively. Compared with Transformer-based methods, our ECEformer outperforms SToKE by 14.09% in ğ‘€ğ‘…ğ‘…, by 9.72% in ğ»ğ‘–ğ‘¡ğ‘ @1, by 19.51% in ğ»ğ‘–ğ‘¡ğ‘ @3, and by 18.6% in ğ»ğ‘–ğ‘¡ğ‘ @10, respectively. This indi- cates that our model can effectively learn the unified representations from frequently changing historical events in datasets characterized by fine-grained temporal granularity. Based on the comparative results from the ICEWS (i.e., ICEWS05-15/18/14), ECEformer consis- tently surpasses all the baselines on the ğ‘€ğ‘…ğ‘…, ğ»ğ‘–ğ‘¡ğ‘ @1, and ğ»ğ‘–ğ‘¡ğ‘ @3 metrics. However, it lags behind the second-best model in ğ»ğ‘–ğ‘¡ğ‘ @10. This situation also occurs in the YAGO11K and Wikidata12K. This suggests that although our model demonstrates high precision in predictions, there is still room for improvement in its recall capa- bility. Besides, compared to other datasets, our method shows a relatively slight performance improvement on YAGO11K. This is attributed to 1) the issue of missing timestamps in the YAGO11K SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Table 4: Ablation study on different module combinations on ICEWS14 and Wikidata12K. ğ‘‡ğ‘ƒrepresents the time pre- diction task. The best results are marked in bold. dataset, and 2) the NeuSTIP method additionally introduces Allen algebra [31] to refine temporal rules and designs a specialized time prediction evaluation function to capture temporal information. Nevertheless, the performance gain achieved by our method indi- cates that the proposed masked time prediction task can mitigate challenges arising from low data quality. To analyze the contribution of ECEformer components, we validate them and their variants on the ICEWS14 and Wikidata12K: (1) using only ECER to embed individual query-specific quadruples (without contextual information); (2) combining ECER and MCKR without conducting the time prediction task; (3) combining ECER and MCKR while undertaking the time prediction task (i.e., the complete ECEformer). The detailed results are listed in Table 4. From the results of ECER, we observe that by utilizing the Trans- former architecture, our model can surpass most geoinformation- based models (such as TGeomE). This validates our viewpoint, which emphasizes sufficiently exploring the internal structure and semantic relationships within individual quadruples. Comparing the results of the ECER and ECEformer (ğ‘¤/ğ‘œTP), we observe that incorporating contextual information significantly improves the modelâ€™s reasoning accuracy on Wikidata12K. However, this integra- tion within ICEWS14 slightly diminishes performance. Specifically, ECEformer (ğ‘¤/ğ‘œTP) outperforms ECER by 2.63% on Wikidata12K, but underperforms by 1.20% on ICEWS14 in terms of ğ‘€ğ‘…ğ‘…. This phe- nomenon is attributed to the finer temporal granularity of ICEWS14 compared to Wikidata12K, where the frequent historical events in the context exacerbate spurious correlations. Without the inter- vention of the time prediction task, the model is more prone to erroneous estimations. Comparing the results of ECEformer (ğ‘¤/ğ‘œ TP) and ECEformer, we observe consistent performance enhance- ments on both datasets after executing the time prediction task. This is especially pronounced in ICEWS14, where the implementation of the time prediction task results in a notable augmentation of the ğ‘€ğ‘…ğ‘…, surpassing the performance of the ECER. Specifically, ECE- former outperforms ECER by 1.10%, and it outperforms ECEformer (ğ‘¤/ğ‘œTP) by 2.30% in terms of ğ‘€ğ‘…ğ‘…. These findings confirm the effectiveness of our proposed temporal information enhancement strategy in prompting the model to leverage temporal information within its context, thereby effectively mitigating the impacts of Table 5: Sensitivity analysis of ECEformer with different number of unit layers. ğ‘and ğ‘€represent the number of ECER units and MCKR units, respectively. (b) Random marking rate ğ›¾ Figure 3: Sensitivity analysis of weight coefficients ğœ†and random masking rate ğ›¾on ICEWS14 and Wikidata12K. As mentioned in Implementation Details, we explore the perfor- mance fluctuations of ECEformer caused by the structural control parameters ğ‘and ğ‘€, as well as the weight coefficient ğœ†. For struc- tural control parameters ğ‘and ğ‘€, we set the base of one parameter to {1, 2}, while the other is adjusted in multiples, specifically to 1Ã—, 2Ã—, and 3Ã—. As shown in Table 5, the results highlight the maximum value in each group in bold. Our observations reveal that different structural settings do introduce perturbations to the model, with the disturbance being relatively minor at ğ‘: ğ‘€= 1 : 2. Based on this finding, we configure the architecture of ECEformer as ğ‘= 3 and ğ‘€= 6 across all datasets. For the weight coefficient ğœ†, we record the modelâ€™s performance across a range of {0.5, 1.0, 1.5, 2.0, 2.5}, and plotted these results in Figure 3a. The performance curve trend suggests that excessively weighting the time prediction task can Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA detrimentally affect the link prediction task, thereby reducing over- all performance. Therefore, setting the weight coefficient between 0.5 and 1.0 offers a balanced improvement in model performance. In addition, to investigate the impact of the masking mechanism in the time prediction task on model performance, we report the results of each batch of data under varying masking rates, with details as well as illustrated in Figure 3b. Specifically, we employ a fixed masking rate ğ›¾during the training phase, randomly selecting ğ›¾Ã— batch_size samples per batch to mask their timestamps. We observe that as the masking rate increases, the model performance correspondingly improves. Additionally, compared to Wikidata12K, the performance of ICEWS14 exhibits greater fluctuations with varying masking rates. This observation indicates that datasets with finer temporal granularity are more sensitive to variations in masking rate. Notably, our final model is trained using a fully In this paper, we address the challenge of completing missing fac- tual elements in TKG reasoning by introducing the ECEformer, a novel end-to-end Transformer-based reasoning model. ECEformer primarily comprises two key components: the evolutionary chain of events representation learning (ECER) and the mixed-context knowledge reasoning (MCKR). The ECER, utilizing the Transformer encoder, thoroughly explores the structural and semantic infor- mation of historical events corresponding to each branch in the evolutionary chain of events (ECE). The ECE is composed of neigh- borhood subgraphs of entity nodes unfolded in chronological order. The MCKR based on MLP relies on the channel and patch mixing strategy to facilitate the learning of contextual information interac- tions within the ECE. Moreover, an additional time prediction task and a time masking mechanism are employed to force the model to assimilate temporal information from the context. Comparative ex- periments with state-of-the-art methods validate the superiority of our approach. Furthermore, additional ablation studies demonstrate the effectiveness of each module proposed in our ECEformer. This research was supported by National Science and Technology Major Project (2022ZD0119204), National Science Fund for Dis- tinguished Young Scholars (62125601), National Natural Science Foundation of China (62076024, 62172035). [1] Ralph Abboud, Ä°smail Ä°lkan Ceylan, Thomas Lukasiewicz, and Tommaso Salva- tori. 2020. BoxE: A Box Embedding Model for Knowledge Base Completion. In Advances in Neural Information Processing Systems. 9649â€“9661. [2] Maristella Agosti, Stefano Marchesin, and Gianmaria Silvello. 2020. Learning unsupervised knowledge-enhanced representations to reduce the semantic gap in information retrieval. ACM Transactions on Information Systems 38, 4 (2020), [3] Farah Atif, Ola El Khatib, and Djellel Difallah. 2023. BeamQA: Multi-hop Knowl- edge Graph Question Answering with Sequence-to-Sequence Prediction and Beam Search. In Proceedings of the International ACM SIGIR Conference on Re- search and Development in Information Retrieval. 781â€“790. [4] Antoine Bordes, Nicolas Usunier, Alberto GarcÃ­a-DurÃ¡n, Jason Weston, and Ok- sana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems. 2787â€“2795. [5] Kai Chen, Ye Wang, Yitong Li, and Aiping Li. 2022. RotateQVS: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion. In Proceedings of the Annual Meeting of the Asso- ciation for Computational Linguistics. 5843â€“5857. [6] Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, and Yangfeng Ji. 2021. HittER: Hierarchical Transformers for Knowledge Graph Embeddings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. [7] Xiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, and Huajun Chen. 2022. Hybrid Transformer with Multi- Level Fusion for Multimodal Knowledge Graph Completion. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information [8] Shib Sankar Dasgupta, Swayambhu Nath Ray, and Partha P. Talukdar. 2018. HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. [9] Songgaojun Deng, Huzefa Rangwala, and Yue Ning. 2020. Dynamic knowl- edge graph based multi-event forecasting. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1585â€“1595. [10] Yifu Gao, Yongquan He, Zhigang Kan, Yi Han, Linbo Qiao, and Dongsheng Li. 2023. Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion. In Findings of the Association for Computational Linguistics. 417â€“430. [11] Rishab Goel, Seyed Mehran Kazemi, Marcus A. Brubaker, and Pascal Poupart. 2020. Diachronic Embedding for Temporal Knowledge Graph Completion. In Proceedings of the AAAI Conference on Artificial Intelligence. 3988â€“3995. [12] Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. 2020. DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion. In Proceedings of the Conference on Empirical Methods in Natural [13] Zhiwei Hu, Victor Gutierrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Pan. 2022. Transformer-based Entity Typing in Knowledge Graphs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. 5988â€“6001. [14] Zhiwei Hu, VÃ­ctor GutiÃ©rrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Z Pan. 2023. HyperFormer: Enhancing entity and relation interaction for hyper-relational knowledge graph completion. In Proceedings of the ACM International Conference on Information and Knowledge Management. 803â€“812. [15] Prachi Jain, Sushant Rathi, Soumen Chakrabarti, et al. 2020. Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. 3733â€“3747. [16] Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren. 2020. Recurrent Event Net- work: Autoregressive Structure Inferenceover Temporal Knowledge Graphs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. [17] Jaehun Jung, Jinhong Jung, and U Kang. 2021. Learning to walk across time for interpretable temporal knowledge graph completion. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 786â€“795. [18] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimiza- tion. In Proceedings of the International Conference on Learning Representations. [19] TimothÃ©e Lacroix, Guillaume Obozinski, and Nicolas Usunier. 2020. Tensor Decompositions for Temporal Knowledge Base Completion. In Proceedings of the International Conference on Learning Representations. [20] Julien Leblay and Melisachew Wudage Chekol. 2018. Deriving Validity Time in Knowledge Graph. In Companion of the International World Wide Web Conferences. [21] Yujia Li, Shiliang Sun, and Jing Zhao. 2022. Tirgn: time-guided recurrent graph network with local-global historical patterns for temporal knowledge graph rea- soning. In Proceedings of the International Joint Conference on Artificial Intelligence. [22] Yancong Li, Xiaoming Zhang, Bo Zhang, and Haiying Ren. 2022. Each snapshot to each space: Space adaptation for temporal knowledge graph completion. In Proceedings of the International Semantic Web Conference. 248â€“266. [23] Zixuan Li, Xiaolong Jin, Saiping Guan, Wei Li, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng. 2021. Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. 4732â€“4743. [24] Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen, Yuanzhuo Wang, and Xueqi Cheng. 2021. Temporal knowledge graph reasoning based on evolutional representation learning. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. [25] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Si- hang Zhou, and Xinwang Liu. 2023. Learn from Relational Correlations and Periodic Events for Temporal Knowledge Graph Reasoning. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information [26] Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp. 2022. Tlogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs. In Proceedings of the AAAI conference on artificial intelligence, SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA [27] Johannes Messner, Ralph Abboud, and Ä°smail Ä°lkan Ceylan. 2022. Temporal Knowledge Graph Completion Using Box Embeddings. In Proceedings of the AAAI Conference on Artificial Intelligence. 7779â€“7787. [28] Guanglin Niu and Bo Li. 2023. Logic and commonsense-guided temporal knowl- edge graph completion. In Proceedings of the AAAI Conference on Artificial Intelli- [29] Ali Sadeghian, Mohammadreza Armandpour, Anthony Colas, and Daisy Zhe Wang. 2021. ChronoR: Rotation Based Temporal Knowledge Graph Embedding. In Proceedings of the AAAI Conference on Artificial Intelligence. 6471â€“6479. [30] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling Relational Data with Graph Con- volutional Networks. In Proceedings of the European Semantic Web Conference, [31] Ishaan Singh, Navdeep Kaur, Garima Gaur, et al. 2023. NeuSTIP: A Neuro- Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. [32] Haohai Sun, Shangyi Geng, Jialun Zhong, Han Hu, and Kun He. 2022. Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs. In Proceedings of the Conference on Empirical Methods in Natural Language [33] Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, and Kun He. 2021. TimeTrav- eler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. [34] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. 2021. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems 34 (2021), 24261â€“24272. [35] Jiapu Wang, Boyue Wang, Junbin Gao, Xiaoyan Li, Yongli Hu, and Baocai Yin. 2023. QDN: A Quadruplet Distributor Network for Temporal Knowledge Graph Completion. IEEE Transactions on Neural Networks and Learning Systems (2023), [36] Jiapu Wang, Boyue Wang, Meikang Qiu, Shirui Pan, Bo Xiong, Heng Liu, Linhao Luo, Tengfei Liu, Yongli Hu, Baocai Yin, et al. 2023. A survey on temporal knowledge graph completion: Taxonomy, progress, and prospects. arXiv preprint [37] Ruijie Wang, Zheng Li, Dachun Sun, Shengzhong Liu, Jinning Li, Bing Yin, and Tarek Abdelzaher. 2022. Learning to sample and aggregate: Few-shot reasoning over temporal knowledge graphs. Advances in Neural Information Processing [38] Chengjin Xu, Yung-Yu Chen, Mojtaba Nayyeri, and Jens Lehmann. 2021. Temporal knowledge graph completion using a linear temporal regularizer and multivector embeddings. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. [39] Chenjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Yazdi, and Jens Lehmann. 2020. Temporal knowledge graph completion based on time series gaussian embedding. In Proceedings of the International Semantic Web Conference. 654â€“ [40] Chengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens Lehmann. 2020. TeRo: A Time-aware Knowledge Graph Embedding via Tem- poral Rotation. In Proceedings of the International Conference on Computational [41] Chengjin Xu, Mojtaba Nayyeri, Yung-Yu Chen, and Jens Lehmann. 2023. Geo- metric Algebra Based Embeddings for Static and Temporal Knowledge Graph Completion. IEEE Transactions on Knowledge and Data Engineering 35, 5 (2023), [42] Zezhong Xu, Peng Ye, Hui Chen, Meng Zhao, Huajun Chen, and Wen Zhang. 2022. Ruleformer: Context-aware Rule Mining over Knowledge Graph. In Proceedings of the International Conference on Computational Linguistics. 2551â€“2560. [43] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for knowledge graph completion. arXiv preprint arXiv:1909.03193 (2019). [44] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive learning for recommendation. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. 1294â€“1303. [45] Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, and Liang Wang. 2023. Learning latent relations for temporal knowledge graph reasoning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. 12617â€“12631. [46] Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, and Liang Wang. 2023. Learning Long-and Short-term Representations for Temporal Knowledge Graph Reasoning. In Proceedings of the ACM Web Conference. 2412â€“2422. [47] Sensen Zhang, Xun Liang, Hui Tang, and Zhenyu Guan. 2023. Hybrid Interaction Temporal Knowledge Graph Embedding Based on Householder Transformations. In Proceedings of the ACM International Conference on Multimedia. 8954â€“8962. [48] Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing Xu, Wenting Song, and Huajun Chen. 2023. Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer. In Proceedings of the ACM Web Conference. [49] Shangfei Zheng, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wei Chen, and Lei Zhao. 2023. DREAM: Adaptive Reinforcement Learning based on At- tention Mechanism for Temporal Knowledge Graph Reasoning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in 